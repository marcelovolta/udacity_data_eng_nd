{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marcelovolta/repos/udacity_data_eng_nd/project_02/workspace\n"
     ]
    }
   ],
   "source": [
    "# checking your current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get your current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "# join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "\n",
    "    # Uncomment next line if you want to check the paths retrieved    \n",
    "    #print(file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list \n",
    "for f in file_path_list:\n",
    "\n",
    "# reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    " # extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "# uncomment the code below if you would like to get total number of rows \n",
    "#print(len(full_data_rows_list))\n",
    "# uncomment the code below if you would like to check to see what the list of event data rows will look like\n",
    "#print(full_data_rows_list)\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_full csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables. \n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin writing your Apache Cassandra code in the cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should make a connection to a Cassandra instance your local machine \n",
    "# (127.0.0.1)\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "try: \n",
    "    cluster = Cluster(['127.0.0.1'], port=6000) #If you have a locally installed Apache Cassandra instance\n",
    "    session = cluster.connect()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Keyspace \n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS sparkify \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    session.set_keyspace('sparkify')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'event_datafile_new.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to create tables to run the following queries. Remember, with Apache Cassandra you model the database tables on the queries you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create queries to ask the following three questions of the data\n",
    "\n",
    "### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "\n",
    "### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the tables in Cassandra according to the following table that lists the requirements for each query: \n",
    "\n",
    "| Query | Filter_01 | Filter_02 | Sort | Field_01 | Field_02 | Field_03 |    \n",
    "| --- | --- | --- |\n",
    "| 1 | sessionid | iteminsession | None | artist | song | length |\n",
    "| 2 | userid | sessionid | iteminsession | artist | song | firstname + lastname |\n",
    "| 3 | song | None | None | firstname + lastname | None | None |\n",
    "\n",
    "The approach followed is to use the column(s) included in the filter definition for the partition key, and the column(s) used for sorting, in the clustering key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query_01 - Table creation: events_session_item\n",
    "\n",
    "drop_query = 'DROP TABLE IF EXISTS events_session_item'\n",
    "create_query = 'CREATE TABLE IF NOT EXISTS events_session_item '\n",
    "create_query = create_query + '(sessionid int, iteminsession int, artist text, song text, length decimal, PRIMARY KEY (sessionid, iteminsession))'\n",
    "\n",
    "try:\n",
    "    session.execute(drop_query)\n",
    "    session.execute(create_query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.9 s, sys: 1.26 s, total: 6.16 s\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# events_session_item: Insertion of records\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO events_session_item (sessionid, iteminsession, artist, song, length) \"\n",
    "        query = query + \"VALUES (%s, %s, %s, %s, %s)\"\n",
    "        session.execute(query, (int(line[8]), int(line[3]), line[0], line[9], float(line[5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a SELECT to verify that the data have been inserted into each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: Faithless - Song: Music Matters (Mark Knight Dub) - Length: 495.3073\n"
     ]
    }
   ],
   "source": [
    "# run Query_01 \n",
    "# The events_session_item table has a partition key composed of the sessionid and iteminsession columns as these\n",
    "# are included in the filter. These two are enough to uniquely identify a row and also there is no sorting needed\n",
    "# in the query (we will typically obtain a single row), so clustering key was not defined\n",
    "\n",
    "select_query = 'SELECT artist, song, length FROM events_session_item '\n",
    "select_query = select_query + 'WHERE sessionId = 338 AND itemInSession = 4'\n",
    "rows = session.execute(select_query)\n",
    "\n",
    "for row in rows:\n",
    "    print ('Artist: {0} - Song: {1} - Length: {2}'.format(row.artist, row.song, row.length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COPY AND REPEAT THE ABOVE THREE CELLS FOR EACH OF THE THREE QUESTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query_02 - Table creation: events_user_session\n",
    "\n",
    "drop_query_2 = 'DROP TABLE IF EXISTS events_user_session'\n",
    "create_query = 'CREATE TABLE IF NOT EXISTS events_user_session '\n",
    "create_query = create_query + '(userid int, sessionid int, iteminsession int, artist text, song text, name text, PRIMARY KEY ((userid, sessionid), iteminsession))'\n",
    "\n",
    "try:\n",
    "    session.execute(drop_query_2)\n",
    "    session.execute(create_query)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.82 s, sys: 1.25 s, total: 6.07 s\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# events_user_session: Insertion of records\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO events_user_session (userId, sessionId, itemInSession, artist, song, name) \"\n",
    "        query = query + \"VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "        session.execute(query, (int(line[10]), int(line[8]), int(line[3]), line[0], line[9], (line[1] + ' ' + line[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: Down To The Bone - Song: Keep On Keepin' On - User: Sylvie Cruz\n",
      "Artist: Three Drives - Song: Greece 2000 - User: Sylvie Cruz\n",
      "Artist: Sebastien Tellier - Song: Kilometer - User: Sylvie Cruz\n",
      "Artist: Lonnie Gordon - Song: Catch You Baby (Steve Pitron & Max Sanna Radio Edit) - User: Sylvie Cruz\n"
     ]
    }
   ],
   "source": [
    "#run Query_02\n",
    "# For the events_user_session, the partition key is composed by userid and sessionid as these are included in the\n",
    "# where clause. Also, since during a session a user can listen to more than one song we need to add another column \n",
    "# to the primary key to make it unique. Since we also need to sort the results by iteminsession, that column is\n",
    "# the natural choice for a clustering key\n",
    "\n",
    "select_query = 'SELECT artist, song, name FROM events_user_session '\n",
    "select_query = select_query + 'WHERE userId = 10 AND sessionId = 182'\n",
    "rows = session.execute(select_query)\n",
    "\n",
    "for row in rows:\n",
    "    print ('Artist: {0} - Song: {1} - User: {2}'.format(row.artist, row.song, row.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query_03 table creation: events_song\n",
    "\n",
    "drop_query_3 = 'DROP TABLE IF EXISTS events_song'\n",
    "create_query = 'CREATE TABLE IF NOT EXISTS events_song '\n",
    "create_query = create_query + '(song text, userid int, name text, PRIMARY KEY (song, userid))'\n",
    "\n",
    "try:\n",
    "    session.execute(drop_query_3)\n",
    "    session.execute(create_query)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.34 s, sys: 1.27 s, total: 6.61 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#events_song: insertion of records\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO events_song (song, userid, name) \"\n",
    "        query = query + \"VALUES (%s, %s, %s)\"\n",
    "        session.execute(query, (line[9], int(line[10]), (line[1] + ' ' + line[4])))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacqueline Lynch\n",
      "Tegan Levine\n",
      "Sara Johnson\n"
     ]
    }
   ],
   "source": [
    "#run Query_03\n",
    "# The events_song table will be queried with a song name in the where clause. To make the rows unique we have added \n",
    "# the userid as a clustering key, since we only need each song-user combination to appear once\n",
    "\n",
    "select_query = 'SELECT name FROM events_song '\n",
    "select_query = select_query + \"WHERE song = 'All Hands Against His Own'\"\n",
    "rows = session.execute(select_query)\n",
    "\n",
    "for row in rows:\n",
    "    print (row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the table\n",
    "drop_query = 'DROP TABLE IF EXISTS events_session_item'\n",
    "create_query = 'CREATE TABLE IF NOT EXISTS events_session_item '\n",
    "create_query = create_query + '(sessionid int, iteminsession int, artist text, song text, length decimal, PRIMARY KEY (sessionid, iteminsession))'\n",
    "\n",
    "try:\n",
    "    session.execute(drop_query)\n",
    "    session.execute(create_query)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.08 s, sys: 1.27 s, total: 6.35 s\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#first method: Using direct inserts and Python File I/O capabilities\n",
    "\n",
    "#Extraction and Load\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        query = \"INSERT INTO events_session_item (sessionid, iteminsession, artist, song, length) \"\n",
    "        query = query + \"VALUES (%s, %s, %s, %s, %s)\"\n",
    "        session.execute(query, (int(line[8]), int(line[3]), line[0], line[9], float(line[5])))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Tests\n",
    "\n",
    "To try to improve the loading time, I considered using the BATCH statement in Cassandra, but found out that this is an antipattern if used for rows that will be recorded in more than one node and it is only recommended when the records to be inserted have the same value of Primary Key. \n",
    "I tried the Pandas approach both to open the file and use it in the insertion, plus the PreparedStatems feature (method #02 and finally a combination of the first method (file I/O and PreparedStatements), which I called method #03. \n",
    "To compare the efficiency of each method, I used the %%time magic. \n",
    "\n",
    "Results are shown below. The best method is the first one, the original approach used in the demos of the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the table\n",
    "drop_query = 'DROP TABLE IF EXISTS events_session_item'\n",
    "create_query = 'CREATE TABLE IF NOT EXISTS events_session_item '\n",
    "create_query = create_query + '(sessionid int, iteminsession int, artist text, song text, length decimal, PRIMARY KEY (sessionid, iteminsession))'\n",
    "\n",
    "try:\n",
    "    session.execute(drop_query)\n",
    "    session.execute(create_query)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.01 s, sys: 1.36 s, total: 10.4 s\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Method 02: Use Pandas and prepared Statements\n",
    "#(Reference: https://medium.com/swlh/building-a-python-data-pipeline-to-apache-cassandra-on-a-docker-container-fc757fbfafdd)\n",
    "\n",
    "#Open file in Pandas\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "#Prepare the CQL statement\n",
    "query_insert = \"INSERT INTO events_session_item \" \n",
    "query_insert = query_insert + \"(sessionid, iteminsession, artist, song, length) \"\n",
    "query_insert = query_insert + \"VALUES (?, ?, ?, ?, ?)\"\n",
    "\n",
    "prepared = session.prepare(query_insert)\n",
    "\n",
    "#Run Insertion\n",
    "for index, row in df.iterrows():\n",
    "    session.execute(prepared\n",
    "                    , (row['sessionId']\n",
    "                    , row['itemInSession']\n",
    "                    , row['artist']\n",
    "                    , row['song']\n",
    "                    , row['length']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the table\n",
    "drop_query = 'DROP TABLE IF EXISTS events_session_item'\n",
    "create_query = 'CREATE TABLE IF NOT EXISTS events_session_item '\n",
    "create_query = create_query + '(sessionid int, iteminsession int, artist text, song text, length decimal, PRIMARY KEY (sessionid, iteminsession))'\n",
    "\n",
    "try:\n",
    "    session.execute(drop_query)\n",
    "    session.execute(create_query)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.65 s, sys: 1.26 s, total: 6.91 s\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Method 03: No Pandas, prepared statement\n",
    "\n",
    "prepared = session.prepare(query_insert)\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "        session.execute(prepared, (int(line[8]), int(line[3]), line[0], line[9], float(line[5])))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x115c55438>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(drop_query)\n",
    "session.execute(drop_query_2)\n",
    "session.execute(drop_query_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dand37]",
   "language": "python",
   "name": "conda-env-dand37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
